##We use bi-level updating frequency N = 5000 for Cheetah, Hammer, Button Press, Drawer Open and Window Open, 
#N = 1000 for Walker, N = 3000 for Quadruped, 
#and N = 10000 for Door Open and Sweep Into
defaults:
  - agent: sac_mrn
    
# this needs to be specified manually
experiment: MRN

# reward learning
segment: 50
activation: leaky_relu
num_seed_steps: 50000
num_unsup_steps: 9000
num_meta_steps: 5000
num_interact: 5000
reward_lr: 0.0003
reward_batch: 128
reward_update: 50
feed_type: 0
reset_update: 100
topK: 5
ensemble_size: 3
max_feedback: 1400
large_batch: 10
label_margin: 0.0
teacher_beta: -1
teacher_gamma: 1
teacher_eps_mistake: 0
teacher_eps_skip: 0
teacher_eps_equal: 0
outer_weight: 0.1
rm_train_batch_size: 128

leverage_prior_data: 1
prior_data_amount: 50000
prior_data_path: 0

# scheduling
reward_schedule: 0
num_train_steps: 1e6
replay_buffer_capacity: ${num_train_steps}
rm_train_batch_size: 128
#ablation
transfer_replay_buffer: 1
transfer_reward_model: 1
base_dir: temp
prior_data_path: 0
prior_data_amount: 59001
load_agent: 1
# evaluation config
eval_frequency: 5000
num_eval_episodes: 10
device: cuda
# logger
log_frequency: 5000
log_save_tb: true

# video recorder
save_video: false

# setups
seed: 1

# Environment
env: dog_stand
gradient_update: 1
env_domain: DMCONTROL
wandb_mode: online
# hydra configuration
hydra:
  name: ${env}
  run:
    dir: ./exp/${env_domain}/${env}/SDP_MRN/H${diag_gaussian_actor.params.hidden_dim}_L${diag_gaussian_actor.params.hidden_depth}_lr${agent.params.actor_lr}/teacher_b${teacher_beta}_g${teacher_gamma}_m${teacher_eps_mistake}_s${teacher_eps_skip}_e${teacher_eps_equal}/init${num_seed_steps}_unsup${num_unsup_steps}_inter${num_interact}_seg${segment}_act${activation}_Rlr${reward_lr}_Rupdate${reward_update}_en${ensemble_size}_sample${feed_type}_large_batch${large_batch}_schedule_${reward_schedule}_label_smooth_${label_margin}/${experiment}_maxfeed${max_feedback}_Rbatch${reward_batch}_meta${num_meta_steps}_seed${seed}